{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "from keras.models import Sequential\r\n",
    "from keras.layers import Dense\r\n",
    "from keras.layers import Dropout\r\n",
    "from keras.layers import InputLayer\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.preprocessing import LabelEncoder\r\n",
    "from sklearn.preprocessing import OneHotEncoder\r\n",
    "\r\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import the data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "df = pd.read_csv('data.csv')\r\n",
    "\r\n",
    "X = df.drop(columns = ['0']).copy()\r\n",
    "y = df['0']\r\n",
    "\r\n",
    "integer_encoded = LabelEncoder().fit_transform(y)\r\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\r\n",
    "y_onehot = OneHotEncoder(sparse=False).fit_transform(integer_encoded)\r\n",
    "\r\n",
    "\r\n",
    "# Split data into testing, training and validation data\r\n",
    "X_train, X_rem, y_train, y_rem = train_test_split(X, y_onehot, train_size=0.8)\r\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_rem,y_rem, test_size=0.5)\r\n",
    "\r\n",
    "classes = ['a', 'b', 'c', 'd', 'e', 'f', 'g']\r\n",
    "num_classes = len(classes)\r\n",
    "num_features = len(X.columns)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define and compile the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "model = Sequential()\r\n",
    "model.add(InputLayer(input_shape=(num_features,)))\r\n",
    "model.add(Dense(20, activation= 'relu' ))\r\n",
    "model.add(Dropout(0.2))\r\n",
    "model.add(Dense(10, activation= 'relu' ))\r\n",
    "model.add(Dense(10, activation= 'relu' ))\r\n",
    "model.add(Dense(num_classes, activation= 'softmax' ))\r\n",
    "model.compile(loss= 'categorical_crossentropy' , optimizer= 'adam' , metrics=[ 'accuracy' ])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fit the model to our data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "model.fit(X_train, y_train,\r\n",
    "          epochs=20,\r\n",
    "          batch_size= 10)\r\n",
    "score = model.evaluate(X_test, y_test, batch_size=10)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n",
      "1227/1227 [==============================] - 2s 2ms/step - loss: 1.3607 - accuracy: 0.4757\n",
      "Epoch 2/20\n",
      "1227/1227 [==============================] - 2s 1ms/step - loss: 0.6636 - accuracy: 0.7452\n",
      "Epoch 3/20\n",
      "1227/1227 [==============================] - 2s 1ms/step - loss: 0.4580 - accuracy: 0.8341\n",
      "Epoch 4/20\n",
      "1227/1227 [==============================] - 2s 1ms/step - loss: 0.3638 - accuracy: 0.8658\n",
      "Epoch 5/20\n",
      "1227/1227 [==============================] - 2s 2ms/step - loss: 0.3064 - accuracy: 0.8888\n",
      "Epoch 6/20\n",
      "1227/1227 [==============================] - 2s 2ms/step - loss: 0.2611 - accuracy: 0.9056\n",
      "Epoch 7/20\n",
      "1227/1227 [==============================] - 3s 2ms/step - loss: 0.2114 - accuracy: 0.9253\n",
      "Epoch 8/20\n",
      "1227/1227 [==============================] - 2s 2ms/step - loss: 0.1840 - accuracy: 0.9380\n",
      "Epoch 9/20\n",
      "1227/1227 [==============================] - 2s 2ms/step - loss: 0.1563 - accuracy: 0.9451\n",
      "Epoch 10/20\n",
      "1227/1227 [==============================] - 2s 2ms/step - loss: 0.1417 - accuracy: 0.9514\n",
      "Epoch 11/20\n",
      "1227/1227 [==============================] - 2s 1ms/step - loss: 0.1227 - accuracy: 0.9608\n",
      "Epoch 12/20\n",
      "1227/1227 [==============================] - 2s 1ms/step - loss: 0.1069 - accuracy: 0.9642\n",
      "Epoch 13/20\n",
      "1227/1227 [==============================] - 2s 1ms/step - loss: 0.0969 - accuracy: 0.9663\n",
      "Epoch 14/20\n",
      "1227/1227 [==============================] - 2s 2ms/step - loss: 0.0969 - accuracy: 0.9679\n",
      "Epoch 15/20\n",
      "1227/1227 [==============================] - 3s 2ms/step - loss: 0.0806 - accuracy: 0.9754\n",
      "Epoch 16/20\n",
      "1227/1227 [==============================] - 2s 1ms/step - loss: 0.0845 - accuracy: 0.9714\n",
      "Epoch 17/20\n",
      "1227/1227 [==============================] - 1s 942us/step - loss: 0.0720 - accuracy: 0.9767\n",
      "Epoch 18/20\n",
      "1227/1227 [==============================] - 1s 890us/step - loss: 0.0761 - accuracy: 0.9747\n",
      "Epoch 19/20\n",
      "1227/1227 [==============================] - 1s 893us/step - loss: 0.0713 - accuracy: 0.9766\n",
      "Epoch 20/20\n",
      "1227/1227 [==============================] - 1s 935us/step - loss: 0.0687 - accuracy: 0.9780\n",
      "154/154 [==============================] - 0s 721us/step - loss: 0.0060 - accuracy: 0.9987\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "import mediapipe as mp\r\n",
    "import cv2\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import string"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\r\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\r\n",
    "mp_hands = mp.solutions.hands\r\n",
    "\r\n",
    "update_time = 10\r\n",
    "save_time = 500"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "def flatten_landmarks(hand_landmarks):\r\n",
    "    keypoints = []\r\n",
    "    for data_point in hand_landmarks.landmark:\r\n",
    "        keypoints.append(data_point.x)\r\n",
    "        keypoints.append(data_point.y)\r\n",
    "        keypoints.append(data_point.z)\r\n",
    "    return keypoints\r\n",
    "    \r\n",
    "def transform_array(array):\r\n",
    "    return list(np.array(array) * (1 / max(array)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "# For webcam input:\r\n",
    "cap = cv2.VideoCapture(1)\r\n",
    "with mp_hands.Hands(\r\n",
    "    max_num_hands=1,\r\n",
    "    min_detection_confidence=0.5,\r\n",
    "    min_tracking_confidence=0.5) as hands:\r\n",
    "\r\n",
    "  i = 0\r\n",
    "  while cap.isOpened():\r\n",
    "    success, image = cap.read()\r\n",
    "    if not success:\r\n",
    "      print(\"Ignoring empty camera frame.\")\r\n",
    "      # If loading a video, use 'break' instead of 'continue'.\r\n",
    "      continue\r\n",
    "\r\n",
    "    # Flip the image horizontally for a later selfie-view display, and convert\r\n",
    "    # the BGR image to RGB.\r\n",
    "    image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\r\n",
    "    # To improve performance, optionally mark the image as not writeable to\r\n",
    "    # pass by reference.\r\n",
    "    image.flags.writeable = False\r\n",
    "    results = hands.process(image)\r\n",
    "\r\n",
    "    key = cv2.waitKey(update_time)\r\n",
    "    i = i + update_time\r\n",
    "    # Draw the hand annotations on the image.\r\n",
    "    image.flags.writeable = True\r\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\r\n",
    "    if results.multi_hand_landmarks:\r\n",
    "        hand_landmarks = results.multi_hand_landmarks[0]\r\n",
    "        if i >= save_time:\r\n",
    "          i = 0\r\n",
    "          flat_keypoints = flatten_landmarks(hand_landmarks)\r\n",
    "          flat_keypoints = transform_array(flat_keypoints)\r\n",
    "          \r\n",
    "          prediction = model.predict([flat_keypoints])\r\n",
    "          prediction_label = classes[np.argmax(prediction)]\r\n",
    "          print(f'Predicted letter: {prediction_label}')\r\n",
    "          \r\n",
    "        mp_drawing.draw_landmarks(\r\n",
    "            image,\r\n",
    "            hand_landmarks,\r\n",
    "            mp_hands.HAND_CONNECTIONS,\r\n",
    "            mp_drawing_styles.get_default_hand_landmarks_style(),\r\n",
    "            mp_drawing_styles.get_default_hand_connections_style())\r\n",
    "            \r\n",
    "    if key == ord(' '):\r\n",
    "      break\r\n",
    "            \r\n",
    "\r\n",
    "    cv2.imshow('MediaPipe Hands', image)\r\n",
    "    \r\n",
    "cap.release()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Predicted letter: d\n",
      "Predicted letter: g\n",
      "Predicted letter: g\n",
      "Predicted letter: g\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit"
  },
  "interpreter": {
   "hash": "05670d60c7d382de5afdc06056b1506cbf442734a10c1850b42e7fcab5ceb50e"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}